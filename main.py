from interface import choice, clear, green, home, open_response, red
from subcommand import run, send
import sys
from datetime import datetime

INSTALL_STR = "Install a new model"

INSTALL_DESCR = '''Enter the name of a model to install.
For a complete list of options, see:
    https://ollama.com/search

you can also install models manually using
ollama pull <model name>
'''

def fmt_timestamp():
    now = datetime.now()
    return f"{now.year:04d}-{now.month:02d}-{now.day:02d}_{now.hour:02d}-{now.minute:02d}-{now.second:02d}"

LOG_FILE = f"logs/chat_log_{fmt_timestamp()}.txt"

def log_message(role: str, message: str):
    with open(LOG_FILE, "a") as f:
        f.write(f"[{role}]\n")
        f.write(message)
        f.write("\n")

def log(txt):
    with open(LOG_FILE, "a") as f:
        f.write(txt)
        f.write("\n")

def system_msg(txt):
    print(green(f"[{txt}]"))

def multiline_input():
    system_msg("multiline input. press Ctrl+D (EOF) when done.")
    lines = []
    print(">>>")
    try:
        res = sys.stdin.read()
        system_msg("input recorded. please wait while response is generated...")
        return res
    except KeyboardInterrupt:
        print("\nchat closed.")
        exit()

def check_version(supress=False):
    out, err = run("ollama -v")
    if (err != ""):
        if not supress:
            print("failed to install ollama.")
        return False
    else:
        if not supress:
            print(f"found ollama version {out.split(" ")[-1]}")
        return True

def try_install_model(model_name):
    print(f"installing model {model_name}...")
    print("this may take a while.")
    res = send("pull", http_verb="POST", params={"stream": False, "model": model_name})
    keys = res.keys()
    status = res["status"] if "status" in keys else False
    if not status:
        print(red("failed to install model. Double check the model name and your internet connection."))
    return status

def main():
    # make sure that ollama is installed and running
    if not check_version(supress=True):
        print("installing/updating ollama...")
        run("curl -fsSL https://ollama.com/install.sh | sh")
    if not check_version():
        return
    run("ollama serve")

    # ask the user to choose which model to use, or give them the option to install new ones
    model = ""
    while True:
        opts = [model["name"] for model in send("tags")["models"]]
        opts.append(INSTALL_STR)
        model = choice(opts, num_keys=True, prompt="Choose a model from the list below.")
        if (model == INSTALL_STR):
            open_response(INSTALL_DESCR, try_install_model)
        else:
            break
    if model == "":
        return

    log("INFO:")
    log("generated by ollama-client (link)")
    log(f"model = {model}")
    log(f"timestamp = {datetime.now()}\n")
    log("BEGIN CHAT LOG:")

    clear()
    home()
    print(f"now chatting with {model}.")
    while True:
        msg = multiline_input()
        log_message("user", msg)
        res = send(
            "chat",
            http_verb="POST",
            params={
                "stream": False,
                "model": model,
                "messages": [
                    {
                        "role": "user",
                        "content": msg
                    }
                ],
            }
        )["message"]
        log_message(res["role"], res["content"])
        print(res["content"])

if __name__ == "__main__":
    main()
